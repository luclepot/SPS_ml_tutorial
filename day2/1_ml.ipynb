{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SPS machine learning tutorial, day 1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lMdBJaMEKcFp"
      },
      "source": [
        "# SPS Machine Learning Tutorial, Part 2\n",
        "---\n",
        "January 29/30, 2020 // Luc Le Pottier, University of Michigan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NIkdFAQqLMAW"
      },
      "source": [
        "##### first: a cool viz which didn't fit last time\n",
        "just because I didn't have time, here's another cool way of visualizing dimensional data.\n",
        "\n",
        "Here we will show a cool method of transforming sound files (.wav), which are in the amplitude-time domain, to the frequency-time domain. \n",
        "\n",
        "First we get some .wav files: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWIoGFJTospW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://github.com/luclepot/SPS_ml_tutorial/raw/master/data/synths/SYNTH-FancyPoly800Dazed.wav https://github.com/luclepot/SPS_ml_tutorial/raw/master/data/synths/SYNTH-PWMRezzzzzzz.wav\n",
        "!ls *.wav"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdd2Q4lGospc",
        "colab_type": "text"
      },
      "source": [
        "Then we can make what is called a spectrogram; basically, an image of the behavior of the .wav file in time, as a function of intensity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bb3202jCKVAu",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.signal\n",
        "import scipy.io.wavfile\n",
        "import matplotlib.pyplot as plt \n",
        "import matplotlib.colors as plt_colors\n",
        "import glob\n",
        "\n",
        "filenames = glob.glob('SYNTH-*.wav')\n",
        "for path in filenames:\n",
        "    \n",
        "    name = path.split('-')[-1].replace('.wav', '')\n",
        "    sample_rate, data = scipy.io.wavfile.read(path)\n",
        "    f,t,Sxx = scipy.signal.spectrogram(data, fs=sample_rate, return_onesided=True)\n",
        "\n",
        "    plt.figure(figsize=(4*len(t)/len(f),4))\n",
        "    plt.pcolormesh(t, f, Sxx, cmap='CMRmap', norm=plt_colors.LogNorm())\n",
        "    plt.colorbar().set_label('intensity')\n",
        "\n",
        "    plt.ylabel('frequency (hz)')\n",
        "    plt.xlabel('time (s)')\n",
        "    plt.gca().set_aspect(t.max()/f.max() * len(f)/len(t))\n",
        "    plt.title(name)\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIV3VTGeospm",
        "colab_type": "text"
      },
      "source": [
        "## 1. Processing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0aG2bpFospp",
        "colab_type": "text"
      },
      "source": [
        "As I probably made exceedingly clear, building a machine learning model is in large part about getting good data. Therefore, lets grab some data first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb5Y05jWospr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/luclepot/SPS_ml_tutorial/raw/master/data/jets.h5 -q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDuah8EWospy",
        "colab_type": "text"
      },
      "source": [
        "Nice. We can load the data using h5, which we should have learned yesterday. Here's how to do it, but I encourage anyone to try it themselves first!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxYXJHkyospz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "\n",
        "f = h5py.File('jets.h5', 'r')\n",
        "\n",
        "print(list(f.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1-xb4ztosp8",
        "colab_type": "text"
      },
      "source": [
        "We can see there are a few datasets in this h5 file. We just want the `jet_features` column for now, which appears to be a group:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJpP1VQGosp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f['jet_features'])\n",
        "print(list(f['jet_features'].keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY-cTTFDosqI",
        "colab_type": "text"
      },
      "source": [
        "We can load the entirety of these datasets into memory because they are small, with the `labels` key being the columns of the set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUwvdUo-osqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_raw = f['jet_features']['data'][:] \n",
        "columns = f['jet_features']['labels'][:]\n",
        "data_raw.shape, columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9OZ1TDCosqQ",
        "colab_type": "text"
      },
      "source": [
        "##### What is this data? \n",
        "Good question. This is data on individual jets at the LHC, with 9 attributes each per jet. \n",
        "\n",
        "##### What is a jet?\n",
        "also a good question. Jets are analysis objects produced at LHC detectors.\n",
        "\n",
        "the heavy and unstable results of high-energy particle collisions, i.e. the higgs boson or a top quark, decay rapidly after they are produced. So rapidly, in fact, that they do not make it anywhere close to the actual detectors!\n",
        "\n",
        "Instead, these (and other) particles *shower*, or decay into various other hadrons (hadronization!). These hadrons then often decay as well, forming a cone of decay products. \n",
        "\n",
        "This cone of decay products - usually hadrons, electrons, and photons - is then intercepted by detectors, and it must be inferred what original particle generated these resultant particles. \n",
        "\n",
        "This is a *very* difficult problem, and an active field of research (my field of research!!)\n",
        "\n",
        "##### What are we doing with this data?\n",
        "These data points are from QCD (quark and gluon-produced) jets with very specific characteristics; namely, we have the leading and subleading (first and second highest transverse momentum, or $p_T$) jets for a number of events.\n",
        "\n",
        "A good way to learn machine learning is to build a model. In this case, we can try to build a model to detect gluon jets from others. Using the $P,~T,~E$ characterization from the lecture slides, we can define this task as:\n",
        "* *$P$*: high true positive rate, low false positive rate \n",
        "* *$T$*: discriminating between gluon and non-gluon jets\n",
        "* *$E$*: jet samples containing some observables; in our case, these are\n",
        "    * $p_T$, jet transverse momentum\n",
        "    * $M$, jet invariant mass\n",
        "    * Fraction of charged particles in jet\n",
        "    * $PTD$, a complex jet quantity that I will not attempt to describe\n",
        "    * $Axis^2$, ditto above\n",
        "    * $E$, jet energy\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "MUP85ckKosqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# reshape the data so it is all jets, and decode the feature labels\n",
        "data = pd.DataFrame(data_raw.reshape((7406*2, 9)), columns=map(lambda x: x.decode('utf-8'), columns))\n",
        "\n",
        "# isolate gluon tags, make them the y-axis\n",
        "y = data['Flavor'].isin([9, 21])\n",
        "data['is_gluon'] = y\n",
        "y = y.astype(int)\n",
        "\n",
        "# remove flavor\n",
        "data.drop(['Flavor'], axis=1, inplace=True)\n",
        "\n",
        "# plot according to gluon status\n",
        "sns.pairplot(data, hue='is_gluon', vars=[c for c in data.columns if c != 'is_gluon'])\n",
        "data.drop('is_gluon', axis=1, inplace=True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5C4YfZ2osqg",
        "colab_type": "text"
      },
      "source": [
        "##### **These categories look similar!!**\n",
        "\n",
        "That's fine - a lot of times structural information is not easily represented by human-readable visualizations. What we CAN see, though, is that $\\eta$ and $\\phi$ don't seem to provide much value - in fact, they are both randomly distributed (to within the limits of our small sample size)!\n",
        "\n",
        "Lets remove them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqpLespPosqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.drop(['Eta', 'Phi'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t04mudN9tZ_D",
        "colab_type": "text"
      },
      "source": [
        "## **2. Building a Model**\n",
        "\n",
        "For demonstration, we want to first try and use a nice neural network to discriminate between these two categories. While We are aware that this might not work, let's go through the steps of building one!\n",
        "\n",
        "First, we import keras. This is a *wrapper* library for tensorflow (a tensor library). Keras makes implementing neural networks much easier, but it is less configurable (will be important when you are making super-ultra-mega-GAN-CONVnets in a month) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGkVikkOtZpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import warnings\n",
        "import seaborn\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m9B2z4CuInQ",
        "colab_type": "text"
      },
      "source": [
        "Keras is great. The main parts of it, which you will need to know, are:\n",
        "* **`keras.layers`**: the layer-containing module, in which prebuilt layers are contained\n",
        "* **`keras.models`**: model-containing part of keras; how you build your models. \n",
        "\n",
        "Below I will build a simple neural network, using layer and model blocks from these libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "navdXL0DuHvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris = sns.load_dataset('iris')\n",
        "x = iris.drop('species', axis=1)\n",
        "encoding_dict = {name: i for i,name in enumerate(iris.species.unique())}\n",
        "y = iris['species'].apply(lambda i: encoding_dict[i])\n",
        "y = keras.utils.np_utils.to_categorical(y)\n",
        "\n",
        "# input layer with shape equivalent to the number of features in our dataset\n",
        "input_layer = keras.layers.Input(shape=(len(x.columns),), name='inputs')\n",
        "\n",
        "# hidden layer with 20 neurons, and relu activation\n",
        "hidden_layer1 = keras.layers.Dense(10, activation='relu', name='hidden_1')\n",
        "\n",
        "# output layer with softmax activation\n",
        "output_layer = keras.layers.Dense(y.shape[1], activation='softmax', name='output')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP3hFzCxvjWF",
        "colab_type": "text"
      },
      "source": [
        "NOTE that we have only defined the layer blocks, and not connected any tensors!\n",
        "\n",
        "Tensors can be connected to one another by feeding them into each other as function arguments. This is a bit confusing, so watch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH4aOEunviey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current = input_layer\n",
        "current = hidden_layer1(current) # feed input into hidden layer\n",
        "output = output_layer(current) # "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsROmA63wuiA",
        "colab_type": "text"
      },
      "source": [
        "Now we have chained the tensors together, and have an input and output. We can now connect these in a meaningful way with the `Model` class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsBbptB-tUaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn = keras.models.Model(inputs=input_layer, outputs=output)\n",
        "nn.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg-z3jkZw_ik",
        "colab_type": "text"
      },
      "source": [
        "In addition to the 'summary' function, we can print a cool graphic of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCJw_fzgw-yR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(nn, to_file='arch.png', show_layer_names=True, show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iQFqFo5xgap",
        "colab_type": "text"
      },
      "source": [
        "## **3: Training a model**\n",
        "\n",
        "This is the fun (and sometimes annoying part): training the model. The model first has to be compiled, and then fit.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd2K2lKwxfjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.compile(optimizer=keras.optimizers.adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGc8X5vnyTDh",
        "colab_type": "text"
      },
      "source": [
        "To fit the model, we need to split out data into training and testing samples. This is because models - especially big ones - are prone to memorize all samples in a small training set, and ours is not very big. \n",
        "\n",
        "A model which works on memorization, not generalization, is usually very bad and cannot do well on a test set. For this reason, we should split it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIR-6-w2xYi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# we gotta split our labels too!\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8)\n",
        "\n",
        "print(x_train.shape, x_test.shape)\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNY1U6nDzBbJ",
        "colab_type": "text"
      },
      "source": [
        "Now we can train! For this we use the `fit` function of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBcdIdghy2XP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = nn.fit(x_train, y_train, epochs=200, validation_split=0.2, verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m07jGTZCzxKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig,axs = plt.subplots(2,1,figsize=(8,12))\n",
        "for key,value in history.history.items():\n",
        "    if key.endswith('loss'):\n",
        "        axs[0].plot(value, label=key)\n",
        "    else:\n",
        "        axs[1].plot(value, label=key)\n",
        "\n",
        "axs[0].legend()\n",
        "axs[1].legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLXn_P613-5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score, accuracy = nn.evaluate(x_test, y_test)\n",
        "print('test score: ', score)\n",
        "print('test accuracy: ', accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzX223pEZ2nW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}