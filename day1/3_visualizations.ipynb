{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lMdBJaMEKcFp"
   },
   "source": [
    "# SPS Python Tutorial, Day 1\n",
    "---\n",
    "*January 29/30, 2020*\n",
    "\n",
    "Luc Le Pottier, University of Michigan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0N6lP2a8SlK4"
   },
   "source": [
    "## **3: visualization**\n",
    "Now we cover some of the most useful data visualization and plotting utilities through a series of examples.\n",
    "\n",
    "we can summarize the basic utility of a few of the most useful plotting libraries as so:\n",
    "- **`matplotlib`** module:\n",
    "    - provides a baseline interface for all plotting\n",
    "    - very customizable\n",
    "    - can be annoying to get everything *just right*\n",
    "    - scripts might be long\n",
    "- **`seaborn`** wrapper module for `matplotlib`:\n",
    "    - very intuitive\n",
    "    - works with `pandas` quite nicely\n",
    "    - automatically finds pretty formatting options\n",
    "    - more confusing to customize than `matplotlib`\n",
    "- **`pandas`** plotting functions within the `DataFrame` object\n",
    "    - data \n",
    "    - slow\n",
    "- **`corner`** module and other custom plot modules\n",
    "    - provide quick implementations of hard-to-write functions; in this case, corner plots\n",
    "    - huge positive side to the open-source nature of python \n",
    "    \n",
    "    \n",
    "This next part of the tutorial will provide examples of advanced plotting capabilities for each utility. We will also bring various scientific computing libraries into the fold, i.e. `scipy`, `sklearn`, and more. \n",
    "\n",
    "This is done in the hopes of sharpening our understanding of data preprocessing and analysis, which **is** the most important part of machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: custom distributions\n",
    "\n",
    "We can create custom probability distribution functions and sample from them using the `scipy` module. \n",
    "\n",
    "This is helpful in \n",
    "- monte carlo simulations\n",
    "- sample generation for training data\n",
    "- sample augmentation for training data\n",
    "\n",
    "We'll generate the blackbody radiation pdf\n",
    "$\\frac{x^3}{e^{x} - 1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats \n",
    "import scipy.interpolate\n",
    "import scipy.integrate \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom distributions\n",
    "x = np.linspace(0.0, 20, int(1e4))\n",
    "\n",
    "T_array = [0.4, 1.0, 2.0]\n",
    "\n",
    "# custom distribution function\n",
    "def p(v, T):\n",
    "    return (v**3)/(np.exp(x/T) - 0.1)\n",
    "\n",
    "# function to approximate pdf and cdf\n",
    "def pdf_and_cdf(p, x, *args, **kwargs):\n",
    "    \n",
    "    # integrate function to get norm constant\n",
    "    norm_constant = scipy.integrate.simps(p(x, *args, **kwargs), x)\n",
    "    \n",
    "    # calculate pdf and cdp for x values\n",
    "    pdf = p(x, *args, **kwargs)/norm_constant\n",
    "    cdf = np.cumsum(pdf)\n",
    "    cdf /= max(cdf)\n",
    "    \n",
    "    return pdf, cdf\n",
    "\n",
    "# generate samples from a cdf\n",
    "def generate_samples(n, cdf, x):\n",
    "    intf = scipy.interpolate.interp1d(cdf, x)\n",
    "    u = np.random.uniform(intf.x.min(), intf.x.max(), int(n))\n",
    "    return intf(u)\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "for i,T in enumerate(T_array):\n",
    "    pdf, cdf = pdf_and_cdf(p, x, T)\n",
    "    plt.hist(generate_samples(100000, cdf, x), bins=20, density=1, histtype='step', label='T = {}'.format(T), color=colors[i])\n",
    "    plt.plot(x, pdf, color=colors[i])\n",
    "\n",
    "plt.xlabel('frequency')\n",
    "plt.ylabel('pdf')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 resampling from existing distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also draw from existing histograms, effectively re-sampling. First we grab the `iris` dataset, which we will use later, and make a histogram of the sepal_width variable, using the pandas interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# pandas dataframe type\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "iris.sepal_width.hist(bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, but lets augment it! We can generate a rough pdf using the bins and edges of the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins, xpdf = np.histogram(iris.sepal_width, bins=10, density=1)\n",
    "xplot = edges[:-1] + np.diff(edges)/2\n",
    "pdf = np.asarray([0,] + list(bins))\n",
    "\n",
    "\n",
    "cdf = np.cumsum(pdf)\n",
    "cdf /= max(cdf)\n",
    "\n",
    "def generate_samples(n, cdf, x):\n",
    "    intf = scipy.interpolate.interp1d(cdf, x)\n",
    "    u = np.random.uniform(intf.x.min(), intf.x.max(), int(n))\n",
    "    return intf(u)\n",
    "\n",
    "iris.sepal_width.hist(bins=10, density=1, histtype='step', label='original')\n",
    "plt.hist(generate_samples(5000, cdf, xpdf), bins=20, density=1, histtype='step', label='generated')\n",
    "plt.plot(xplot, bins, label='estimated pdf')\n",
    "plt.xticks(np.arange(2, 5, 0.5), np.arange(2, 5, 0.5))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 pair and corner plots\n",
    "\n",
    "These plots are good for visualizing high-dimensional datasets, in particle physics and other fields.\n",
    "\n",
    "Image a situation where you have individual samples, characterized by some mid-dimensional set of observables, such as the iris dataset. Corner/pair plots will show you the distributions and co-distributions of each variable, and each combination of variables, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, seaborn: this provides a very readable visualization of the data, indicating differences in distributions between species and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2VczHIkWimu"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "sns.pairplot(iris, hue='species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use corner, which is more suitable to massive sample sizes and struggles under these circumstances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corner.corner(iris.drop('species', axis=1), bins=15, color='tab:blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a more illuminating usage of corner, we can make our own dataset of multivariate gaussians:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "colab_type": "code",
    "id": "PVdX83-LbWRh",
    "outputId": "18c151e5-c6ce-4c83-df7f-499c5fc37066"
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "means = np.random.uniform(-2, 2, n)\n",
    "cov = np.random.uniform(0, 1, size=(n,n))\n",
    "cov = np.maximum(np.mean([cov, cov.T], axis=0), np.identity(n)) \n",
    "\n",
    "big_data = np.random.multivariate_normal(mean=means, cov=cov.T, size=10000)\n",
    "_ = corner.corner(big_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "Yx_mzxnIKX5i",
    "outputId": "b9fb96f0-6fee-4cb0-b291-218cd9d046a4"
   },
   "source": [
    "We can also make weird looking double gaussians, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "gen = lambda n : np.random.multivariate_normal([0.5, 0, -1], np.identity(3), size=n)\n",
    "data1,data2,data3 = gen(N),gen(N),gen(N)\n",
    "data2[:int(N/2),0] += np.random.normal(3, 0.5, size=int(N/2))\n",
    "data3[int(N/2):,1] += np.random.normal(-3, 0.5, size=int(N/2))\n",
    "\n",
    "# fig = corner.corner(data1, bins=40, color='tab:blue')\n",
    "corner.corner(data2, bins=40, color='tab:orange')\n",
    "# corner.corner(data3, bins=40, color='tab:green', fig=fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 scatter plots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get fancy plotting scatter plots. Let's reload the california housing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "housing = pd.read_csv('sample_data/california_housing_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot x,y coordinates of blocks, with their size characterized by physical size and their color characterized by median home price. This is all done via the pandas functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind='scatter', x='longitude', y='latitude',\n",
    "            c='median_house_value', figsize=(10,10),\n",
    "             cmap='jet', s=housing['population']/100, \n",
    "             label='population', alpha=0.35, colorbar=True\n",
    "            )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.gca().set_aspect(1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 correlation plots\n",
    "\n",
    "If you have a high number of observables, pandas also provides a `corr` function to determine which are most useful for a possible training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = housing.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SPS machine learning tutorial, day 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
