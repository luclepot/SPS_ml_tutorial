{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lMdBJaMEKcFp"
   },
   "source": [
    "# SPS Python Tutorial, Day 1\n",
    "---\n",
    "*January 29/30, 2020*\n",
    "\n",
    "Luc Le Pottier, University of Michigan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIkdFAQqLMAW"
   },
   "source": [
    "## **2: data storage and loading**\n",
    "This section will introduce tensor libraries and compressed file format libraries, such as\n",
    "- the `numpy` module:\n",
    "    - `ndarray` data storage object and associated functions\n",
    "    - basic numerical constants and mathematical functions\n",
    "    - loading and saving datasets in `npz` format\n",
    "\n",
    "- the `pandas` module:\n",
    "    - `DataFrame` object and associated functions\n",
    "    - data loading and processing functionality\n",
    "\n",
    "- the `h5py` module:\n",
    "    - what an HDF5 file is/why it is useful\n",
    "    - loading and saving datasets to HDF5 format with `h5py`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1: numpy**\n",
    "\n",
    "We'll start with (probably) the most fundamental python library, `numpy`. I won't pretend that anyone is unfamiliar with numpy. but I will go over a few things. \n",
    "\n",
    "**what is a python tensor?**\n",
    "\n",
    "Tensors in Python are objects with shape $n_1\\times n_2 \\times ... \\times n_m$, containing $\\prod_{i=0}^m n_i$ elements total. They are important to us because they are\n",
    "\n",
    "- fixed in size (very different from the C++ vector, for instance)\n",
    "- extremely fast to do math with\n",
    "- *very* useful for machine learning\n",
    "\n",
    "There are plenty of python libraries full of functions and objects which provide easy creation, manipulation, and interpretation of tensors. Numpy, pandas, and many of the machine learning libraries we will see later all fall into this classification. \n",
    "\n",
    "numpy itself is useful because it is \n",
    "- implemented mostly in C and fortran, meaning it is a *lot* faster than python alone (10-100 times)\n",
    "- full of useful but esoteric data processing functions\n",
    "- the proud owner of its own compressed data type\n",
    "\n",
    "lets look some numpy features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bb3202jCKVAu"
   },
   "outputs": [],
   "source": [
    "# the classic pseudonym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### new arrays\n",
    "this can be from zeros, ones, raw memory (empty), random distributions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros\n",
    "z0 = np.zeros((5,10,2))\n",
    "\n",
    "# ones\n",
    "z1 = np.ones((3,2))\n",
    "\n",
    "# empty\n",
    "z_empty = np.empty((10,7))\n",
    "\n",
    "# uniform data\n",
    "uniform = np.random.uniform(3, 4, 10)\n",
    "\n",
    "# gaussian data in any shape\n",
    "gaussian = np.random.normal(loc=1, scale=0.5, size=(3, 5))\n",
    "\n",
    "# ones tensor in the shape of the gaussian tensor\n",
    "gaussian0 = np.ones_like(gaussian)\n",
    "\n",
    "# z_empty might have data in it already\n",
    "z_empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### conversion from other types\n",
    "\n",
    "this can be done from many types; you basically just want to plug into the `np.asarray` function and see. \n",
    "\n",
    "Generators are a special case. this is useful when you want to inspect the output of a generator, i.e. for debugging. Generators are usually used for massive amounts of data; i.e. when you don't want to/can not load all of your data into memory at once, but you still need to analyze / train models on that data.\n",
    "\n",
    "This example is not that case, but is still interesting. Note how the count must be specified, as numpy has fixed-size arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lists\n",
    "a = [1, 5, 2, 3, -1]\n",
    "np_arr = np.asarray(a)\n",
    "\n",
    "# convert from generators\n",
    "def data_generator(N):\n",
    "    for i in range(N):\n",
    "        # some resource intensive calculation\n",
    "        yield i**2 - np.log(i + 1)\n",
    "        \n",
    "n = 50\n",
    "gen = data_generator(n)\n",
    "np_gen = np.fromiter(gen, dtype=float, count=n)\n",
    "np_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vectorizing operations\n",
    "There are many parts of numpy which can provide *huge* time savings (in exchange for memory) by vectorizing for-loops. `np.tile` is specifically useful for this goal.\n",
    "\n",
    "An example of when this might be necessary is if you wanted to calculate all combinations of sums of two vectors, which would return a matrix. This can be done in a for-loop, and vectorized. Both are shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "n1, n2 = 1000, 600\n",
    "\n",
    "v1 = np.random.uniform(0, 10, n1)\n",
    "v2 = np.random.uniform(-5, -1, n2)\n",
    "\n",
    "# for-loop method:\n",
    "t0 = time()\n",
    "for_result = []\n",
    "\n",
    "for i in range(len(v1)):\n",
    "    for_result.append([])\n",
    "    for j in range(len(v2)):\n",
    "        for_result[i].append(v1[i] + v2[j])\n",
    "for_result = np.asarray(for_result)\n",
    "\n",
    "print('{:>15}: {:.8f} s'.format('for-loop method', time() - t0))\n",
    "    \n",
    "# vectorized method:\n",
    "t0 = time()\n",
    "result = np.tile(v1, [len(v2), 1]).T + np.tile(v2, [len(v1), 1])\n",
    "print('{:>15}: {:.8f} s'.format('vector method', time() - t0))\n",
    "\n",
    "print(np.isclose(result, for_result).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the vectorized method is *generally* 1-2 orders of magnitude faster, especially as the vector sizes $n_1$ and $n_2$ grow large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data types\n",
    "\n",
    "we can also save huge datasets using numpys custom data format, `.npy`. \n",
    "\n",
    "We can save one array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data = np.random.normal(loc=5.0, scale=2., size=(10000,200))\n",
    "np.save('test', big_data)\n",
    "\n",
    "reloaded = np.load('test.npy')\n",
    "\n",
    "np.isclose(reloaded, big_data).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can save multiple to a .zip style file, which can then be reloaded as an archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_data = np.empty((100,20))\n",
    "\n",
    "np.savez('test', big_data=big_data, other_data=other_data)\n",
    "\n",
    "archive = np.load('test.npz')\n",
    "np.isclose(archive['other_data'], other_data).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using .npy and .npz files\n",
    "pros:\n",
    "- significantly faster than using .csv files\n",
    "- provides a file archiving system for in-time access of data\n",
    "- lazy-loaded; i.e. loading an archive does not load all files into memory\n",
    "\n",
    "cons:\n",
    "- specific to python & numpy\n",
    "- partial loading must be done at save-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: pandas\n",
    "\n",
    "Pandas is an awesome module build on top of numpy. It provides\n",
    "- category visualization\n",
    "- plotting\n",
    "- fast numpy-based math\n",
    "- builtin file-reading functions\n",
    "\n",
    "... among other things. We will briefly show how useful it can be by constructing a pandas DataFrame, the main matrix-like object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# make random data of size N\n",
    "N = 150\n",
    "year = np.random.randint(1950, 1990, N)\n",
    "\n",
    "income = 1e3*np.round(np.random.normal(loc=62, scale=20, size=N))\n",
    "income[income < 1000] = 1000\n",
    "\n",
    "# some fake relation between income and # cars\n",
    "n_cars = (income**3./(income**3.).mean() + np.random.uniform(-.5, 1.0, N)).astype(int)\n",
    "\n",
    "data = pd.DataFrame({'birth_year': year, 'income': income, 'n_cars': n_cars})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can then do tons of things with the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 10 entries\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific variable access\n",
    "data.birth_year.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counting\n",
    "data.n_cars.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### composite columns\n",
    "It is easy to add columns to a dataframe, like a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['income_per_car'] = data.income/data.n_cars\n",
    "\n",
    "data.income_per_car.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also replace bad entries, i.e. division by zero. We can also fill NaN entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.income_per_car.replace(np.inf, np.nan, inplace=True)\n",
    "data.income_per_car.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0, inplace=True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pandas plotting\n",
    "pandas dataframes have some built-in plotting functionality, though it is slightly limited. Some of the more interesting ones are boxplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = data.boxplot('income', 'n_cars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = data.income_per_car[data.income_per_car > 0].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and regular plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = data.plot('income', 'birth_year', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get more into the plotting later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loading/saving data\n",
    "\n",
    "pandas provides tons of resources for loading and saving data, listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in dir(pd):\n",
    "    if elt.startswith('read_'):\n",
    "        print(elt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this can be super nice when dealing with strange file formats, i.e. excel, clipboard, sql, etc.\n",
    "\n",
    "Lets load up a .csv file (provided by google colab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('sample_data/california_housing_train.csv')\n",
    "housing.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will leave the statistical analysis of this file for the machine learning portion (tomorrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: HDF5 files\n",
    "\n",
    "HDF5 is a file format which essentially fixes the aforementioned numpy file issues. It is\n",
    "\n",
    "- cross-language\n",
    "- able to load any subset of data\n",
    "- very very very very fast\n",
    "- awesome heirarchical structure\n",
    "\n",
    "Lets look at these things in demonstration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### hierarchal structure\n",
    "\n",
    "We can use the test dataset we had before (housing) as an example of this. Say we want to save the median_house_value data only, along with a matrix of latitude and longitude pair coordinates.\n",
    "\n",
    "H5 files have a group/dataset structure. You can create\n",
    "- groups: names directories for datasets\n",
    "- datasets: named tensors of values\n",
    "\n",
    "H5 files are edited in real time and are constantly open, as long as the File object exists. You can also not remake datasets or groups, so this cell can only be run once without deleting the h5 file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API module for h5 files\n",
    "import h5py\n",
    "\n",
    "f = h5py.File('test.h5', 'w')\n",
    "\n",
    "pos_group = f.create_group('position')\n",
    "pos_group.create_dataset('latitude', data=housing.latitude.values)\n",
    "pos_group.create_dataset('longitude', data=housing.longitude.values)\n",
    "\n",
    "f.create_dataset('value', data=housing.median_house_value.values)\n",
    "\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once we have an h5 file, we can inspect its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f.keys())\n",
    "print(f['position'])\n",
    "print(f['position'].keys())\n",
    "print(f['position']['latitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### subset loading\n",
    "\n",
    "Datasets will NOT be loaded unless called for - super useful for big files. To give an example of this, we can load slices of the `latitude` dataset without actually loading the whole thing into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['position']['latitude'][0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in fact, this makes it trival to write a function for loading data in batches for processing. As an example, the following function loads all values in batches of 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_housing_batch(n_per_batch, batch_n, file):\n",
    "    min_ = n_per_batch*batch_n\n",
    "    max_ = min_ + n_per_batch\n",
    "    idx = slice(min_, max_)\n",
    "    ret = pd.DataFrame(index=range(min_, max_, 1))\n",
    "    \n",
    "    for key in file['position'].keys():\n",
    "        ret[key] = file['position'][key][idx]\n",
    "        \n",
    "    ret['value'] = file['value'][idx]\n",
    "\n",
    "    return ret\n",
    "    \n",
    "# 52nd batch of 100 elements\n",
    "batch = get_next_housing_batch(n_per_batch=100, batch_n=52, file=f)\n",
    "batch.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### speed\n",
    "we won't show that directly here, but generally h5 files are both faster and more configurable than the `npz` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2 summary\n",
    "\n",
    "That wraps up the (boring thing) information about data loading and storage. While this isn't the most interesting, it gets **super** important for machine learning things. \n",
    "\n",
    "NEXT, the fun part - plotting!!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SPS machine learning tutorial, day 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
